{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Trinh Pham and Angel"],"metadata":{"id":"6UAN_1S6UrOP"}},{"cell_type":"markdown","source":["# Introduction\n","\n","AWS Deepracer is a machine learning, cloud based, racing simulator that completes a given race track based on the user's reward functions and coding ability. With our first few models, we progressed through based on training time (1hr, 30mins, 3hrs), to see if the training time has a significant impact on the model. To our surprise, the training time has little to no impact on how well the model performs.\n","\n","Throughout our model trainings, we decided to stick with one racetrack, the DBRO Raceway track. With this racetrack in particular, there were obstacles that needed to be passed, such as the sudden wide curves. With each training model, we achieved considerable amount of completion time progress.\n","\n","---\n","\n"],"metadata":{"id":"-iYWPhvYIqoQ"}},{"cell_type":"markdown","source":["# First model\n","In the first model, we implemented the standard 'stay within borders' model without adjusting the given code, and simply trained the model for 1 hour. With the first model we acheived rank *211* and time of *2:17*\n","\n","---\n","\n"],"metadata":{"id":"Jj3zAGOQFwix"}},{"cell_type":"markdown","source":["# Second Model\n","\n","In the second model, since we were on a time crunch, we decided to do a training time of 10 minutes. The short training time reflects how poorly the model performed on the track. Unlike the other models we've trained, this one was off track more frequently, with a slower acceleration rate than the others. The model had a completion time of 05:29s.\n","\n","Since we chose the stay within the border reward function, the model will be rewarded whenever it stays within the border.\n","\n"],"metadata":{"id":"axzxPOuw3G8e"}},{"cell_type":"code","source":["def reward_function(params):\n","    # Example of rewarding the agent to stay inside the two borders of the track\n","\n","    # Read input parameters\n","    all_wheels_on_track = params['all_wheels_on_track']\n","    distance_from_center = params['distance_from_center']\n","    track_width = params['track_width']\n","\n","    # Give a very low reward by default\n","    reward = 1e-3\n","\n","    # Give a high reward if no wheels go off the track and\n","    # the agent is somewhere in between the track borders\n","    if all_wheels_on_track and (0.5*track_width - distance_from_center) >= 0.05:\n","        reward = 1.0\n","\n","    # Always return a float value\n","    return float(reward)\n"],"metadata":{"id":"D2c8SrBBNhjM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Third Model\n","With our third model, we tried different reward functions, the already pre-coded 'prevent zig-zag' reward function, and one additional reward function to increase the model's speed. we set the max speed of the agent to 4 to adjust for the slow start it has. Although we adjsuted for the speed, we still needed some more trial and error checks for which speed max was most optimal. This model had a completion time of 2:20s."],"metadata":{"id":"r95_Oy-vGYrG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"T9kGXASAFoF4"},"outputs":[],"source":["MAX_SPEED = 4\n","def reward_function(params):\n","  #Example of penalize steering, which helps mitigate zig-zag behaviors\n","  speed = params ['speed']\n","  speed_rate = speed / MAX_SPEED\n","  return speed_rate ** 2\n","  #read input parameters\n","  distance_from_center =\n","    params['distance_from_center']\n","track_width = params['track_width']\n","abs_steering = abs(params['steering_angle']) #only need the absolute steering angle\n","\n","#calculate 3 marks that are farther and farther away from the center line\n","marker_1 = 0.1 * track_width\n","marker_2 = 0.25 * track_width\n","marker_3 = 0.5 * track_width\n","\n","#Give higher reward if the car is closer to center line and vice versa\n","if distance_from_center <= marker_1:\n","  reward = 5.0\n","elif distance_from_center <= marker2:\n","  reward = 0.5\n","elif distance_from_center <= marker_3:\n","  reward = 0.3\n","else:\n","  reward = 1e-10  # likely crashed/ close to off track\n","\n","# Steering penality threshold, change the number based on your action space setting\n","ABS_STEERING_THRESHOLD = 15\n","\n","# Penalize reward if the car is steering too much\n","if abs_steering > ABS_STEERING_THRESHOLD:\n","  reward *= 0.8\n","\n","return float(reward)\n"]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"JF7l5LL1MqqU"}},{"cell_type":"markdown","source":["# Fourth Model\n","The fourth model is almost identical to the third model but with a longer testing time (3hrs), including a progress reward, and a different reward pathway (Follow centerline). With a longer training time, and a new approach of a reward system, we hoped to get  better runtime results.\n","\n","In the fourth model, as I'm evaluating the model's performance, I realized that the model wasn't doing better than our first model, where we didn't implement any additional rewards. Within 10 minutes, the model was able to complete the track with ease, however, the speed is still a bit slower than the  expected outcome. With our fourth model we were able to achieve a completion time of 02:39.787\n","\n","---\n","\n"],"metadata":{"id":"_8G_tyPnK1yo"}},{"cell_type":"code","source":["def reward_function(params):\n","    # Example of rewarding the agent to follow center line\n","\n","    # Read input parameters\n","    track_width = params['track_width']\n","    distance_from_center = params['distance_from_center']\n","\n","    # Calculate 3 markers that are at varying distances away from the center line\n","    marker_1 = 0.1 * track_width\n","    marker_2 = 0.25 * track_width\n","    marker_3 = 0.5 * track_width\n","\n","    #a speed reward system that penalizes the model when the speed is slow\n","    MAX_SPEED = 4\n","\n","    def reward(params):\n","        speed = params['speed']\n","        speed_rate = speed / MAX_SPEED\n","        return speed_rate ** 2\n","    #the model is rewarded based on its progress, the further it can go in the track with fewer steps , the higher the reward\n","\n","        progress = params['progress']\n","        steps = params['steps']\n","\n","        return progress / steps\n","    # Give higher reward if the car is closer to center line and vice versa\n","    if distance_from_center <= marker_1:\n","        reward = 1.0\n","    elif distance_from_center <= marker_2:\n","        reward = 0.5\n","    elif distance_from_center <= marker_3:\n","        reward = 0.1\n","    else:\n","        reward = 1e-3 # likely crashed/ close to off track\n","\n","    return float(reward)\n"],"metadata":{"id":"nydwi7cwK20I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Conclusion\n","\n","The more we implemented a new reward function or increased the training time, we realized it didn't make any substantial changes. For significant change to occur, we must dive deeper into the overall code of the model and analyze the track. If we can implement a way to forsee the tracks layout, then we can reward the model better."],"metadata":{"id":"p0wn-YZwQBLJ"}}]}